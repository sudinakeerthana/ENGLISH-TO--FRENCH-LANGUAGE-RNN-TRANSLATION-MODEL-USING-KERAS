{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEad54jnEZJz"
      },
      "source": [
        "## NLP PROJECT: MACHINE TRANSLATION \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKJ29l_uShTe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iC-qsCeQnBc"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hudAl7tRH4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7e4ebf-dddb-4396-fb32-2f3acf7eea5a"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 10627536362974003862\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 5113585898624615581\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLt0GQYsFVe6"
      },
      "source": [
        "**LOADING OF DATASET AND UNDERSTANDING OF DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUCnWRszRKfL"
      },
      "source": [
        "def read_data(filename):\n",
        "            file = open(filename, mode='rt', encoding='utf-8')\n",
        "            text = file.read()\n",
        "            file.close()\n",
        "            return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bArj3jZUm7h"
      },
      "source": [
        "def lines(text):\n",
        "      sents = text.strip().split('\\n')\n",
        "      return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA7vf6BeUrCy"
      },
      "source": [
        "data= read_data(\"/content/drive/My Drive/NMT/small_vocab_en\")\n",
        "english_sentences = lines(data)\n",
        "data1= read_data(\"/content/drive/My Drive/NMT/small_vocab_fr\")\n",
        "french_sentences = lines(data1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZceGWKhTDFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f30c2e0-7004-455b-c268-5bbd6a9177d7"
      },
      "source": [
        "for sample_i in range(5):\n",
        "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('French sample {}:  {}\\n'.format(sample_i + 1, french_sentences[sample_i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sample 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "French sample 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "\n",
            "English sample 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "French sample 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "\n",
            "English sample 3:  california is usually quiet during march , and it is usually hot in june .\n",
            "French sample 3:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "\n",
            "English sample 4:  the united states is sometimes mild during june , and it is cold in september .\n",
            "French sample 4:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
            "\n",
            "English sample 5:  your least liked fruit is the grape , but my least liked is the apple .\n",
            "French sample 5:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E97e6dDVTHx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd4967e-2c1c-466f-9789-c800d3c99cf8"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1823250 English words.\n",
            "227 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
            "\n",
            "1961295 French words.\n",
            "355 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_xTE8g2Fszt"
      },
      "source": [
        "**TOKENIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5PF48R9TZSP"
      },
      "source": [
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)b\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OQ5R7AvFdT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42498660-aa88-4987-f7c9-02d850ae0a9c"
      },
      "source": [
        "text_sentences = [\n",
        "    \"dogs are man's best friend.\",\n",
        "    'He decided to skiing on a frozen lake .',\n",
        "    'Toddlers feeding raccoons.']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dogs': 1, 'are': 2, \"man's\": 3, 'best': 4, 'friend': 5, 'he': 6, 'decided': 7, 'to': 8, 'skiing': 9, 'on': 10, 'a': 11, 'frozen': 12, 'lake': 13, 'toddlers': 14, 'feeding': 15, 'raccoons': 16}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  dogs are man's best friend.\n",
            "  Output: [1, 2, 3, 4, 5]\n",
            "Sequence 2 in x\n",
            "  Input:  He decided to skiing on a frozen lake .\n",
            "  Output: [6, 7, 8, 9, 10, 11, 12, 13]\n",
            "Sequence 3 in x\n",
            "  Input:  Toddlers feeding raccoons.\n",
            "  Output: [14, 15, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl0VQtBFx6g"
      },
      "source": [
        "**PADDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKdIAlYocYYp"
      },
      "source": [
        "def pad(x, length=None):\n",
        "\n",
        "    return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQcqd3B4HDTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6538f591-b385-4154-cdef-c95d0bd4f232"
      },
      "source": [
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 3 4 5]\n",
            "  Output: [1 2 3 4 5 0 0 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [ 6  7  8  9 10 11 12 13]\n",
            "  Output: [ 6  7  8  9 10 11 12 13]\n",
            "Sequence 3 in x\n",
            "  Input:  [14 15 16]\n",
            "  Output: [14 15 16  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7zj5sf4cyo4"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    \n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTUAmRjrdD0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f276d55-db9e-4561-92a1-475bf1953167"
      },
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 15\n",
            "Max French sentence length: 21\n",
            "English vocabulary size: 199\n",
            "French vocabulary size: 344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txnCdzrCdW_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b2650c-2f1b-4b71-da28-4daa32b9c375"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    \n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQjhXet_IEA9"
      },
      "source": [
        "**SIMPLE RNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xPq59bzd1H7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd6e9b8-7cb5-473c-c093-904dbda5482b"
      },
      "source": [
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "       \n",
        "    model = Sequential([LSTM(100,input_shape=(21,1),return_sequences=True),TimeDistributed(Dense(345,activation='softmax'))])\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(0.001),metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "simple_rnn_model.summary()\n",
        "simple_rnn_model.fit(tmp_x, preproc_french_sentences, epochs=10,batch_size=1024,validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 21, 100)           40800     \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 21, 345)           34845     \n",
            "=================================================================\n",
            "Total params: 75,645\n",
            "Trainable params: 75,645\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "108/108 [==============================] - 57s 524ms/step - loss: 3.1345 - accuracy: 0.4261 - val_loss: 2.4353 - val_accuracy: 0.4751\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 56s 522ms/step - loss: 2.1993 - accuracy: 0.5016 - val_loss: 1.9816 - val_accuracy: 0.5407\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 57s 529ms/step - loss: 1.8470 - accuracy: 0.5643 - val_loss: 1.7387 - val_accuracy: 0.5808\n",
            "Epoch 4/10\n",
            "108/108 [==============================] - 62s 576ms/step - loss: 1.6714 - accuracy: 0.5883 - val_loss: 1.6128 - val_accuracy: 0.6015\n",
            "Epoch 5/10\n",
            "108/108 [==============================] - 57s 531ms/step - loss: 1.5696 - accuracy: 0.6043 - val_loss: 1.5275 - val_accuracy: 0.6083\n",
            "Epoch 6/10\n",
            "108/108 [==============================] - 58s 535ms/step - loss: 1.4938 - accuracy: 0.6116 - val_loss: 1.4605 - val_accuracy: 0.6144\n",
            "Epoch 7/10\n",
            "108/108 [==============================] - 58s 536ms/step - loss: 1.4316 - accuracy: 0.6178 - val_loss: 1.4035 - val_accuracy: 0.6229\n",
            "Epoch 8/10\n",
            "108/108 [==============================] - 57s 529ms/step - loss: 1.3811 - accuracy: 0.6265 - val_loss: 1.3591 - val_accuracy: 0.6326\n",
            "Epoch 9/10\n",
            "108/108 [==============================] - 57s 530ms/step - loss: 1.3397 - accuracy: 0.6342 - val_loss: 1.3200 - val_accuracy: 0.6354\n",
            "Epoch 10/10\n",
            "108/108 [==============================] - 57s 525ms/step - loss: 1.3057 - accuracy: 0.6398 - val_loss: 1.2919 - val_accuracy: 0.6393\n",
            "new jersey est parfois calme en l' et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXNGmO8ka1pl"
      },
      "source": [
        "**RNN WITH EMBEDDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPQdT9_ieaMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197f090b-57e8-40fc-c2e6-91018bb0fd36"
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 0.005\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(LSTM(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    \n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 21, 256)           51200     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 21, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 21, 345)           353625    \n",
            "=================================================================\n",
            "Total params: 1,193,305\n",
            "Trainable params: 1,193,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "108/108 [==============================] - 375s 3s/step - loss: 1.6237 - accuracy: 0.6345 - val_loss: 0.5916 - val_accuracy: 0.8216\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 371s 3s/step - loss: 0.4716 - accuracy: 0.8509 - val_loss: 0.3287 - val_accuracy: 0.8910\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 376s 3s/step - loss: 0.3190 - accuracy: 0.8943 - val_loss: 0.2684 - val_accuracy: 0.9089\n",
            "Epoch 4/10\n",
            "108/108 [==============================] - 368s 3s/step - loss: 0.2582 - accuracy: 0.9128 - val_loss: 0.2284 - val_accuracy: 0.9201\n",
            "Epoch 5/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.2303 - accuracy: 0.9213 - val_loss: 0.2109 - val_accuracy: 0.9275\n",
            "Epoch 6/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.2109 - accuracy: 0.9269 - val_loss: 0.1973 - val_accuracy: 0.9312\n",
            "Epoch 7/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.2001 - accuracy: 0.9301 - val_loss: 0.1882 - val_accuracy: 0.9343\n",
            "Epoch 8/10\n",
            "108/108 [==============================] - 369s 3s/step - loss: 0.1914 - accuracy: 0.9326 - val_loss: 0.1855 - val_accuracy: 0.9349\n",
            "Epoch 9/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.1851 - accuracy: 0.9343 - val_loss: 0.1834 - val_accuracy: 0.9360\n",
            "Epoch 10/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.1797 - accuracy: 0.9358 - val_loss: 0.1792 - val_accuracy: 0.9366\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruSYmDLBf9Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533c589b-4af2-4530-e213-4aa72001cdef"
      },
      "source": [
        "print(\"Prediction:\")\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(french_sentences[:1])\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(english_sentences[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Correct Translation:\n",
            "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
            "\n",
            "Original text:\n",
            "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOMcNnf0bcZ1"
      },
      "source": [
        "BIDIRECTIONAL RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG3MKbp3uBf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735e3527-ba8d-40b9-bf51-0a581e8c2f65"
      },
      "source": [
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "       learning_rate = 0.003\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 21, 256)           51200     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 21, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 21, 345)           353625    \n",
            "=================================================================\n",
            "Total params: 1,193,305\n",
            "Trainable params: 1,193,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "108/108 [==============================] - 372s 3s/step - loss: 1.5595 - accuracy: 0.6441 - val_loss: 0.5521 - val_accuracy: 0.8320\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 371s 3s/step - loss: 0.4524 - accuracy: 0.8551 - val_loss: 0.3229 - val_accuracy: 0.8916\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 374s 3s/step - loss: 0.3104 - accuracy: 0.8972 - val_loss: 0.2555 - val_accuracy: 0.9132\n",
            "Epoch 4/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.2580 - accuracy: 0.9133 - val_loss: 0.2245 - val_accuracy: 0.9230\n",
            "Epoch 5/10\n",
            "108/108 [==============================] - 371s 3s/step - loss: 0.2282 - accuracy: 0.9222 - val_loss: 0.2092 - val_accuracy: 0.9279\n",
            "Epoch 6/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.2122 - accuracy: 0.9267 - val_loss: 0.1996 - val_accuracy: 0.9295\n",
            "Epoch 7/10\n",
            "108/108 [==============================] - 375s 3s/step - loss: 0.2008 - accuracy: 0.9298 - val_loss: 0.1919 - val_accuracy: 0.9323\n",
            "Epoch 8/10\n",
            "108/108 [==============================] - 370s 3s/step - loss: 0.1902 - accuracy: 0.9329 - val_loss: 0.1855 - val_accuracy: 0.9352\n",
            "Epoch 9/10\n",
            "108/108 [==============================] - 372s 3s/step - loss: 0.1842 - accuracy: 0.9348 - val_loss: 0.1833 - val_accuracy: 0.9359\n",
            "Epoch 10/10\n",
            "108/108 [==============================] - 371s 3s/step - loss: 0.1783 - accuracy: 0.9362 - val_loss: 0.1810 - val_accuracy: 0.9365\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sjQFuWSbgjz"
      },
      "source": [
        "ENCODER AND DECODER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NTKJzyLvGxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321d17a8-2cdd-4ef4-d561-14231981e9b5"
      },
      "source": [
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 0.001\n",
        "        \n",
        "    model = Sequential()\n",
        "    # Encoder\n",
        "    model.add(LSTM(256, input_shape=input_shape[1:], go_backwards=True))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    # Decoder\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "encdec_rnn_model = encdec_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "encdec_rnn_model.summary()\n",
        "\n",
        "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 256)               264192    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 21, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 21, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 21, 345)           353625    \n",
            "=================================================================\n",
            "Total params: 1,406,297\n",
            "Trainable params: 1,406,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "108/108 [==============================] - 468s 4s/step - loss: 2.7051 - accuracy: 0.4566 - val_loss: 1.9474 - val_accuracy: 0.5341\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 469s 4s/step - loss: 1.7008 - accuracy: 0.5662 - val_loss: 1.4282 - val_accuracy: 0.6094\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 464s 4s/step - loss: 1.4022 - accuracy: 0.6139 - val_loss: 1.2897 - val_accuracy: 0.6432\n",
            "Epoch 4/10\n",
            "108/108 [==============================] - 468s 4s/step - loss: 1.3058 - accuracy: 0.6350 - val_loss: 1.2254 - val_accuracy: 0.6575\n",
            "Epoch 5/10\n",
            "108/108 [==============================] - 478s 4s/step - loss: 1.2457 - accuracy: 0.6486 - val_loss: 1.1905 - val_accuracy: 0.6597\n",
            "Epoch 6/10\n",
            "108/108 [==============================] - 476s 4s/step - loss: 1.1934 - accuracy: 0.6590 - val_loss: 1.1287 - val_accuracy: 0.6722\n",
            "Epoch 7/10\n",
            "108/108 [==============================] - 471s 4s/step - loss: 1.1436 - accuracy: 0.6674 - val_loss: 1.0748 - val_accuracy: 0.6816\n",
            "Epoch 8/10\n",
            "108/108 [==============================] - 470s 4s/step - loss: 1.0907 - accuracy: 0.6773 - val_loss: 1.0251 - val_accuracy: 0.6908\n",
            "Epoch 9/10\n",
            "108/108 [==============================] - 464s 4s/step - loss: 1.0703 - accuracy: 0.6811 - val_loss: 1.0232 - val_accuracy: 0.6916\n",
            "Epoch 10/10\n",
            "108/108 [==============================] - 462s 4s/step - loss: 1.0275 - accuracy: 0.6896 - val_loss: 0.9757 - val_accuracy: 0.7014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0b83b30b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo9wRir-boRX"
      },
      "source": [
        "EMBEDDING + BIDIRECTIONAL ENCODER model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Mt3T8K8a_J"
      },
      "source": [
        "   def model_embie(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \n",
        "    learning_rate = 0.003\n",
        "    \n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    # Embedding\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
        "                         input_shape=input_shape[1:]))\n",
        "    # Encoder\n",
        "    model.add(Bidirectional(LSTM(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    # Decoder\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "959cKXGfHaJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba4507a-0dd3-48f6-8d0a-fcef628597ad"
      },
      "source": [
        "def embie_predictions(x, y, x_tk, y_tk):\n",
        "    # TODO: Train neural network using model_final\n",
        "    model = model_embie(x.shape,y.shape[1],\n",
        "                        len(x_tk.word_index)+1,\n",
        "                        len(y_tk.word_index)+1)\n",
        "    model.summary()\n",
        "    model.fit(x, y, batch_size=1024, epochs=25, validation_split=0.2)\n",
        "\n",
        "    \n",
        "embie_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 15, 128)           25600     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 256)               263168    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 21, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 21, 256)           394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 21, 512)           131584    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 21, 512)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 21, 345)           176985    \n",
            "=================================================================\n",
            "Total params: 991,577\n",
            "Trainable params: 991,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "108/108 [==============================] - 318s 3s/step - loss: 2.4257 - accuracy: 0.4897 - val_loss: 1.6266 - val_accuracy: 0.5737\n",
            "Epoch 2/25\n",
            "108/108 [==============================] - 314s 3s/step - loss: 1.4889 - accuracy: 0.5969 - val_loss: 1.2528 - val_accuracy: 0.6475\n",
            "Epoch 3/25\n",
            "108/108 [==============================] - 315s 3s/step - loss: 1.1946 - accuracy: 0.6608 - val_loss: 0.9874 - val_accuracy: 0.7114\n",
            "Epoch 4/25\n",
            "108/108 [==============================] - 316s 3s/step - loss: 0.9720 - accuracy: 0.7105 - val_loss: 0.8154 - val_accuracy: 0.7444\n",
            "Epoch 5/25\n",
            "108/108 [==============================] - 327s 3s/step - loss: 0.8271 - accuracy: 0.7450 - val_loss: 0.6705 - val_accuracy: 0.7886\n",
            "Epoch 6/25\n",
            "108/108 [==============================] - 313s 3s/step - loss: 0.6784 - accuracy: 0.7887 - val_loss: 0.5302 - val_accuracy: 0.8357\n",
            "Epoch 7/25\n",
            "108/108 [==============================] - 322s 3s/step - loss: 0.5452 - accuracy: 0.8291 - val_loss: 0.4011 - val_accuracy: 0.8768\n",
            "Epoch 8/25\n",
            "108/108 [==============================] - 312s 3s/step - loss: 0.4669 - accuracy: 0.8524 - val_loss: 0.3117 - val_accuracy: 0.9057\n",
            "Epoch 9/25\n",
            "108/108 [==============================] - 310s 3s/step - loss: 0.3418 - accuracy: 0.8936 - val_loss: 0.2467 - val_accuracy: 0.9260\n",
            "Epoch 10/25\n",
            "108/108 [==============================] - 319s 3s/step - loss: 0.2848 - accuracy: 0.9125 - val_loss: 0.2067 - val_accuracy: 0.9393\n",
            "Epoch 11/25\n",
            "108/108 [==============================] - 316s 3s/step - loss: 0.2387 - accuracy: 0.9277 - val_loss: 0.1786 - val_accuracy: 0.9477\n",
            "Epoch 12/25\n",
            "108/108 [==============================] - 316s 3s/step - loss: 0.2084 - accuracy: 0.9373 - val_loss: 0.1655 - val_accuracy: 0.9509\n",
            "Epoch 13/25\n",
            "108/108 [==============================] - 313s 3s/step - loss: 0.1824 - accuracy: 0.9454 - val_loss: 0.1403 - val_accuracy: 0.9589\n",
            "Epoch 14/25\n",
            "108/108 [==============================] - 312s 3s/step - loss: 0.1623 - accuracy: 0.9514 - val_loss: 0.1236 - val_accuracy: 0.9645\n",
            "Epoch 15/25\n",
            "108/108 [==============================] - 311s 3s/step - loss: 0.1495 - accuracy: 0.9549 - val_loss: 0.2210 - val_accuracy: 0.9347\n",
            "Epoch 16/25\n",
            "108/108 [==============================] - 320s 3s/step - loss: 0.1981 - accuracy: 0.9398 - val_loss: 0.1073 - val_accuracy: 0.9691\n",
            "Epoch 17/25\n",
            "108/108 [==============================] - 316s 3s/step - loss: 0.1212 - accuracy: 0.9639 - val_loss: 0.1012 - val_accuracy: 0.9707\n",
            "Epoch 18/25\n",
            "108/108 [==============================] - 320s 3s/step - loss: 0.1119 - accuracy: 0.9665 - val_loss: 0.0949 - val_accuracy: 0.9729\n",
            "Epoch 19/25\n",
            "108/108 [==============================] - 313s 3s/step - loss: 0.1023 - accuracy: 0.9693 - val_loss: 0.0894 - val_accuracy: 0.9738\n",
            "Epoch 20/25\n",
            "108/108 [==============================] - 309s 3s/step - loss: 0.0951 - accuracy: 0.9713 - val_loss: 0.0881 - val_accuracy: 0.9741\n",
            "Epoch 21/25\n",
            "108/108 [==============================] - 312s 3s/step - loss: 0.0919 - accuracy: 0.9723 - val_loss: 0.0832 - val_accuracy: 0.9766\n",
            "Epoch 22/25\n",
            "108/108 [==============================] - 311s 3s/step - loss: 0.0871 - accuracy: 0.9738 - val_loss: 0.0778 - val_accuracy: 0.9780\n",
            "Epoch 23/25\n",
            "108/108 [==============================] - 313s 3s/step - loss: 0.0817 - accuracy: 0.9754 - val_loss: 0.0777 - val_accuracy: 0.9785\n",
            "Epoch 24/25\n",
            "108/108 [==============================] - 312s 3s/step - loss: 0.0764 - accuracy: 0.9769 - val_loss: 0.0744 - val_accuracy: 0.9794\n",
            "Epoch 25/25\n",
            "108/108 [==============================] - 312s 3s/step - loss: 0.0729 - accuracy: 0.9779 - val_loss: 0.0706 - val_accuracy: 0.9805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBLCxnmlHsGd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}